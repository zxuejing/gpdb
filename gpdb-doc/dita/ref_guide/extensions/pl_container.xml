<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="using_plcontainer" xml:lang="en">
  <title id="pz212122">PL/Container Language</title>
  <body>
    <p>Pl/Container enables users to run Greenplum procedural language functions inside a Docker
      container, to avoid security risks associated with executing Python or R code on Greenplum
      segment hosts. This topic covers information about the architecture, installation, and setup
      of PL/Container:</p>
    <ul>
      <li id="pz219023"><xref href="#about_pl_container" type="topic" format="dita"/></li>
      <li id="pz213664"><xref href="#topic3" type="topic" format="dita"/></li>
      <li><xref href="#upgrade_plcontainer" type="topic" format="dita"/></li>
      <li id="pz213668"><xref href="#uninstall_plcontainer" type="topic" format="dita"/>
      </li>
      <li><xref href="#topic_kds_plk_rbb" format="dita"/></li>
    </ul>
    <p>For detailed information about using PL/Container, refer to the sections:</p>
    <p>
      <ul id="ul_ozt_t1j_blb">
        <li><xref href="#topic_resmgmt">PL/Container Resource Management</xref></li>
        <li><xref href="#topic_rh3_p3q_dw">PL/Container Functions</xref></li>
      </ul>
    </p>
    <p>For reference documentation, see:<ul id="ul_r5n_zq4_flb">
        <li><xref href="plcontainer.xml#topic_rw3_52s_dw">plcontainer utility</xref> reference
          page</li>
        <li><xref href="plcontainer-configuration.xml#topic_sk1_gdq_dw">plcontainer configuration
            file</xref> reference page</li>
      </ul></p>
    <p>The PL/Container language extension is available as an open source module. For information
      about the module, see the README file in the GitHub repository at <xref
        href="https://github.com/greenplum-db/plcontainer" format="html" scope="external"
        >https://github.com/greenplum-db/plcontainer</xref></p>
  </body>
  <topic id="about_pl_container" xml:lang="en">
    <title>About the PL/Container Language Extension</title>
    <body>
      <p>The Greenplum Database PL/Container language extension allows users to create and run
        PL/Python or PL/R user-defined functions (UDFs) securely, inside a Docker container. Docker
        provides the ability to package and run an application in a loosely isolated environment
        called a container. For information about Docker, see the <xref
          href="https://www.docker.com" format="html">Docker web site</xref>. </p>
      <p>Running UDFs inside the Docker container ensures that:</p>
      <ul>
        <li> The function execution process takes place in a separate environment and allows
          decoupling of the data processing. SQL operators such as "scan," "filter," and "project"
          are executed at the query executor (QE) side, and advanced data analysis is executed at
          the container side. </li>
        <li>User code cannot access the OS or the file system of the local host.</li>
        <li>User code cannot introduce any security risks.</li>
        <li>Functions cannot connect back to the Greenplum Database if the container is started with
          limited or no network access.</li>
      </ul>
      <section id="plcontainer_arch">
        <title>PL/Container Architecture</title>
        <fig id="pl_container_image">
          <image placement="break" href="../graphics/pl_container_architecture.png" width="650"
            height="550" align="center"/>
        </fig>
        <p><b>Example of the process flow</b>:</p>
        <p>Consider a query that selects table data using all available segments, and transforms the
          data using a PL/Container function. On the first call to a function in a segment
          container, the query executor on the master host starts the container on that segment
          host. It then contacts the running container to obtain the results. The container might
          respond with a Service Provider Interface (SPI) - a SQL query executed by the container to
          get some data back from the database - returning the result to the query executor.</p>
        <p>A container running in standby mode waits on the socket and does not consume any CPU
          resources. PL/Container memory consumption depends on the amount of data cached in global
          dictionaries.</p>
        <p>The container connection is closed by closing the Greenplum Database session that started
          the container, and the container shuts down.</p>
      </section>
    </body>
  </topic>
  <topic id="topic3" xml:lang="en">
    <title id="pz214493">Install PL/Container </title>
    <body>
      <note type="warning">PL/Container is compatible with Greenplum Database 5.2.0 and later.
        PL/Container has not been tested for compatibility with Greenplum Database 5.1.0 or 5.0.0. </note>
      <section>This topic includes how to: <ul>
          <li><xref href="#topic3/install_docker" format="dita" scope="local">Install
            Docker</xref></li>
          <li><xref href="#topic3/install_pl_utility" format="dita">Install PL/Container</xref></li>
          <li><xref href="#topic3/install_docker_images" format="dita">Install the PL/Container
              Docker images</xref></li>
          <li><xref href="#topic3/test_installation" format="dita">Test the PL/Container
              installation</xref></li>
        </ul><p>The following sections describe these tasks in detail.</p>
      </section>
      <section id="requirements">
        <title>Prerequisites</title>
        <ul>
          <li>PL/Container is supported on <ph otherprops="pivotal">Pivotal </ph>Greenplum Database
            5.2.x on Red Hat Enterprise Linux (RHEL) 7.x (or later) and CentOS 7.x (or later).
            PL/Container is not supported on RHEL/CentOS 6.x systems, because those platforms do not
            officially support Docker.<note>PL/Container 1.6.0 (GPDB 5.26) and later supports Docker
              images with Python 3 installed.</note></li>
          <li>The minimum Linux OS kernel version supported is 3.10. To verfiy your kernel release
            use: <codeblock>$ uname -r</codeblock>
          </li>
        </ul>
      </section>
      <section id="install_docker">
        <title>Install Docker</title>
        <p>To use PL/Container you need to install Docker on all Greenplum Database host systems.
          These are the minimum Docker versions that must be installed on Greenplum Database hosts
          (master, primary and all standby hosts):<ul id="ul_z2t_bxd_rbb">
            <li>For PL/Container versions up to 1.5.0 - Docker 17.05</li>
            <li>For PL/Container 1.6.0 and later - Docker 19.03</li>
          </ul></p>
        <p>These instructions show how to set up the Docker service on CentOS 7 but RHEL 7 is a
          similar process. These steps install the docker package and start the Docker service as a
          user with sudo privileges. </p>
        <ol>
          <li>Ensure the user has sudo privileges or is root.</li>
          <li>Install the dependencies required for
            Docker:<codeblock>sudo yum install -y yum-utils device-mapper-persistent-data lvm2</codeblock></li>
          <li>Add the Docker
            repo:<codeblock>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</codeblock></li>
          <li>Update yum cache:<codeblock>sudo yum makecache fast</codeblock></li>
          <li>Install Docker:<codeblock>sudo yum -y install docker-ce</codeblock></li>
          <li>Start Docker daemon:<codeblock>sudo systemctl start docker</codeblock></li>
          <li>On each Greenplum Database host, the <codeph>gpadmin</codeph> user should be part of
            the docker group for the user to be able to manage Docker images and containers. Assign
            the Greenplum Database administrator <codeph>gpadmin</codeph> to the group
              <codeph>docker</codeph>: <codeblock>sudo usermod -aG docker gpadmin</codeblock></li>
          <li>Exit the session and login again to update the privileges.</li>
          <li>Configure Docker to start when the host system
            starts:<codeblock>sudo systemctl enable docker.service</codeblock><codeblock>sudo systemctl start docker.service</codeblock></li>
          <li>Run a Docker command to test the Docker installation. This command lists the currently
            running Docker containers. <codeblock>docker ps</codeblock></li>
          <li>After you install Docker on all Greenplum Database hosts, restart the Greenplum
            Database system to give Greenplum Database access to Docker.
            <codeblock>gpstop -ra</codeblock></li>
        </ol>
        <p>For a list of observations while using Docker and PL/Container, see the <xref
            href="#plc_notes">Notes</xref> section. For a list of Docker reference documentation,
          see <xref href="#topic_kds_plk_rbb">Docker References</xref>.</p>
      </section>
      <section id="plc_notes">
        <title>Docker Notes</title>
        <ul id="ul_j4g_vgs_wbb">
          <li>If a PL/Container Docker container exceeds the maximum allowed memory, it is
            terminated and an out of memory warning is displayed. </li>
          <li>PL/Container does not limit the Docker base device size, the size of the Docker
            container. In some cases, the Docker daemon controls the base device size. For example,
            if the Docker storage driver is devicemapper, the Docker daemon
              <codeph>--storage-opt</codeph> option flag <codeph>dm.basesize</codeph> controls the
            base device size. The default base device size for devicemapper is 10GB. The Docker
            command <codeph>docker info</codeph> displays Docker system information including the
            storage driver. The base device size is displayed in Docker 1.12 and later. For
            information about Docker storage drivers, see the Docker information <xref
              href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-storage-driver"
              format="html" scope="external">Daemon storage-driver</xref>. <p>When setting the
              Docker base device size, the size must be set on all Greenplum Database
            hosts.</p></li>
          <li>
            <p><i>Known issue</i>: </p>
            <p>Occasionally, when PL/Container is running in a high concurrency environment, the
              Docker daemon hangs with log entries that indicate a memory shortage. This can happen
              even when the system seems to have adequate free memory.</p>
            <p>The issue seems to be triggered by the aggressive virtual memory requirement of the
              Go language (golang) runtime that is used by PL/Container, and the Greenplum Database
              Linux server kernel parameter setting for <i>overcommit_memory</i>. The parameter is
              set to 2 which does not allow memory overcommit. </p>
            <p>A workaround that might help is to increase the amount of swap space and increase the
              Linux server kernel parameter overcommit_ratio. If the issue still occurs after the
              changes, there might be memory shortage. You should check free memory on the system
              and add more RAM if needed. You can also decrease the cluster load.</p>
          </li>
        </ul>
      </section>
      <section id="install_pl_utility" otherprops="pivotal">
        <title>Install PL/Container </title>
        <!--Pivotal content-->
        <note otherprops="pivotal">The extension PL/Container 1.1 and later is installed by
            <codeph>gppkg</codeph> as a Greenplum Database extension, while the extension
          PL/Container 1.0 is installed as a Greenplum Database language. </note>
        <p>Install the PL/Container language extension using the <codeph>gppkg</codeph> utility.</p>
        <ol id="ul_w5b_nzp_dw">
          <li>Download the "PL/Container for RHEL 7" package that applies to your Greenplum Database
            version, from the <xref href="https://network.pivotal.io/products/pivotal-gpdb/"
              format="html" scope="external">VMware Tanzu Network</xref>. PL/Container is listed
            under Greenplum Procedural Languages. </li>
          <li>As <codeph>gpadmin</codeph>, copy the PL/Container language extension package to the
            master host.</li>
          <li>Run the package installation
            command:<codeblock>gppkg -i plcontainer-1.5.0-rhel7-x86_64.gppkg</codeblock>For
            PL/Container 1.0, run this
              command.<codeblock>psql -d <varname>your_database</varname> -f $GPHOME/share/postgresql/plcontainer/plcontainer_install.sql</codeblock><p>The
              SQL script registers the language <codeph>plcontainer</codeph> in the database and
              creates PL/Container-specific functions and views.</p></li>
          <li>Source the file
            <codeph>$GPHOME/greenplum_path.sh</codeph>:<codeblock>source $GPHOME/greenplum_path.sh</codeblock></li>
          <li>Make sure Greenplum Database is up and running:<codeblock>gpstate -s</codeblock>If
            it's not, start it:<codeblock>gpstart -a</codeblock></li>
          <li>Restart Greenplum Database:<codeblock>gpstop -ra</codeblock></li>
          <li>Login into one of the available databases, for example:
            <codeblock>psql postgres</codeblock></li>
          <li>Register the PL/Container extension, which installs the <codeph>plcontainer</codeph>
              utility:<codeblock>CREATE EXTENSION plcontainer; </codeblock><p>You'll need to
              register the utility separately on each database that might need the PL/Container
              functionality. </p></li>
        </ol>
      </section>
      <section id="install_docker_images">
        <title>Install PL/Container Docker Images</title>
        <p>Install the Docker images that PL/Container will use to create language specific
          containers to run the UDFs.</p>
        <!--oss only content-->
        <p otherprops="oss-only">The PL/Container open source module contains dockerfiles to build
          Docker images that can be used with PL/Container. You can build a Docker image to run
          PL/Python UDFs and a Docker image to run PL/R UDFs. See the dockerfiles in the GitHub
          repository at <xref href="https://github.com/greenplum-db/plcontainer" format="html"
            scope="external">https://github.com/greenplum-db/plcontainer</xref>.</p>
        <!--Pivotal content-->
        <ul id="ul_h5t_sbz_vkb">
          <li>Download the <codeph>tar.gz</codeph> file that contains the Docker images from <xref
              href="https://network.pivotal.io/products/pivotal-gpdb" scope="external" format="html"
              class="- topic/xref ">Pivotal Network</xref>. <ul id="ul_vsj_pxb_tbb">
              <li><codeph>plcontainer-python3-images-&lt;version>.tar.gz</codeph></li>
              <li><codeph>plcontainer-python-images-&lt;version>.tar.gz</codeph></li>
              <li><codeph>plcontainer-r-images-&lt;version>.tar.gz</codeph></li>
            </ul><p>Each image supports a different language or language version:<ul
                id="ul_epg_t2v_qbb">
                <li>PL/Container for Python 3 - Docker image with Python 3.7 and the Python Data
                  Science Module package installed. <note>PL/Container 1.6.0 and later supports
                    Docker images with Python 3 installed.</note></li>
                <li>PL/Container for Python 2 - Docker image with Python 2.7.12 and the Python Data
                  Science Module package installed.</li>
                <li>PL/Container for R - A Docker image with container with R-3.3.3 and the R Data
                  Science Library package installed. </li>
              </ul></p><p>The Data Science packages contain a set of Python modules or R functions
              and data sets related to data science. For information about the packages, see <xref
                href="../../install_guide/install_python_dsmod.xml" format="dita" scope="peer"
                otherprops="pivotal">Python Data Science Module Package</xref> and <xref
                href="../../install_guide/install_r_dslib.xml" format="dita" scope="peer"
                otherprops="pivotal">R Data Science Library Package</xref>.</p><p>If you require
              different images from the ones provided by Pivotal Greenplum, you can create custom
              Docker images, install the image and add the image to the PL/ Container configuration.
            </p></li>
          <li>
            <p>Use the <codeph>plcontainer </codeph> utility command <codeph>image-add</codeph> to
              install the images on all Greenplum Database hosts where <codeph>-f </codeph>indicates
              the location of the downloaded files: </p>
            <codeblock># Install a Python 2 based Docker image
plcontainer image-add -f /home/gpadmin/plcontainer-python-image-1.6.0-gp6.tar.gz
            
# Install a Python 3 based Docker image
plcontainer image-add -f /home/gpadmin/plcontainer-python3-image-1.6.0-gp6.tar.gz
            
# Install an R based Docker image
plcontainer image-add -f /home/gpadmin/plcontainer-r-image-1.6.0-gp6.tar.gz

</codeblock>
            <p>The utility displays progress information, similar to:</p>
            <codeblock>20200127:21:54:43:004607 plcontainer:mdw:gpadmin-[INFO]:-Checking whether docker is installed on all hosts...
20200127:21:54:43:004607 plcontainer:mdw:gpadmin-[INFO]:-Distributing image file /home/gpadmin/plcontainer-python-images-1.5.0.tar to all hosts...
20200127:21:54:55:004607 plcontainer:mdw:gpadmin-[INFO]:-Loading image on all hosts...
20200127:21:55:37:004607 plcontainer:mdw:gpadmin-[INFO]:-Removing temporary image files on all hosts...</codeblock>
            <p>For more information on <codeph>image-add</codeph> options, visit the <xref
                href="plcontainer.xml#topic_rw3_52s_dw">plcontainer </xref> reference page. </p>
          </li>
          <li>To display the installed Docker images on the local host use:
              <codeblock>$ plcontainer image-list</codeblock><simpletable frame="all"
              relcolwidth="1.0* 1.0* 1.0* 1.0*" id="simpletable_xqr_cdz_vkb">
              <sthead>
                <stentry>REPOSITORY</stentry>
                <stentry>TAG</stentry>
                <stentry>IMAGE ID</stentry>
                <stentry>CREATED</stentry>
              </sthead>
              <strow>
                <stentry> pivotaldata/plcontainer_r_shared</stentry>
                <stentry>devel</stentry>
                <stentry>7427f920669d</stentry>
                <stentry>10 months ago</stentry>
              </strow>
              <strow>
                <stentry> pivotaldata/plcontainer_python_shared</stentry>
                <stentry>devel</stentry>
                <stentry>e36827eba53e</stentry>
                <stentry>10 months ago</stentry>
              </strow>
              <strow>
                <stentry> pivotaldata/plcontainer_python3_shared</stentry>
                <stentry>devel</stentry>
                <stentry>y32827ebe55b</stentry>
                <stentry>5 months ago</stentry>
              </strow>
            </simpletable></li>
          <li><p> Add the image information to the PL/Container configuration file using
                <codeph>plcontainer runtime-add</codeph>, to allow PL/Container to associate
              containers with specified Docker images.</p><p>Use the <codeph>-r</codeph> option to
              specify your own user defined runtime ID name, use the <codeph>-i</codeph> option to
              specify the Docker image, and the <codeph>-l</codeph> option to specify the Docker
              image language. When there are multiple versions of the same docker image, for example
              1.0.0 or 1.2.0, specify the TAG version using ":" after the image name.
            </p><codeblock># Add a Python 2 based runtime
plcontainer runtime-add -r plc_python_shared -i pivotaldata/plcontainer_python_shared:devel -l python
            
# Add a Python 3 based runtime that is supported with PL/Container 1.6.0
plcontainer runtime-add -r plc_python3_shared -i pivotaldata/plcontainer_python3_shared:devel -l python3
            
# Add an R based runtime
plcontainer runtime-add -r plc_r_shared -i pivotaldata/plcontainer_r_shared:devel -l r</codeblock>The
            utility displays progress information as it updates the PL/Container configuration file
            on the Greenplum Database instances. <p>For details on other
                <codeph>runtime-add</codeph> options, see the <xref
                href="plcontainer.xml#topic_rw3_52s_dw">plcontainer</xref> reference page. </p></li>
          <li>Optional: Use Greenplum Database resource groups to manage and limit the total CPU and
            memory resources of containers in PL/Container runtimes. In this example, the Python
            runtime will be used with a preconfigured resource group
              16391:<codeblock>plcontainer runtime-add -r plc_python_shared -i pivotaldata/plcontainer_python_shared:devel -l
 python -s resource_group_id=16391</codeblock><p>For
              more information about enabling, configuring, and using Greenplum Database resource
              groups with PL/Container, see <xref href="#topic_resmgmt">PL/Container Resource
                Management </xref> . </p></li>
        </ul>
        <p>You can now create a simple function to test your PL/Container installation. </p>
      </section>
      <section id="test_installation">
        <title>Test the PL/Container Installation</title>
        <p>List the names of the runtimes your created and added to the PL/Container XML
          file:<codeblock>plcontainer runtime-show
</codeblock></p>
        <p>Which will show a list of all installed
          runtimes:<codeblock>PL/Container Runtime Configuration: 
---------------------------------------------------------
  Runtime ID: plc_python_shared
  Linked Docker Image: pivotaldata/plcontainer_python_shared:devel
  Runtime Setting(s): 
  Shared Directory: 
  ---- Shared Directory From HOST '/usr/local/greenplum-db/./bin/plcontainer_clients' to Container '/clientdir', access mode is 'ro'
---------------------------------------------------------
</codeblock>
        </p>
        <p>You can also view the PL/Container configuration information with the <codeph>plcontainer
            runtime-show -r &lt;runtime_id></codeph> command. You can view the PL/Container
          configuration XML file with the <codeph>plcontainer runtime-edit</codeph> command. </p>
        <p>Use the <codeph>psql</codeph> utility and select an existing database: </p>
        <codeblock>psql postgres;</codeblock>
        <p>If the PL/Container extension is not registered with the selected database, first enable
          it using: </p>
        <codeblock>postgres=# CREATE EXTENSION plcontainer;</codeblock>
        <p>Create a simple function to test your installation; in the example, the function will use
          the runtime <codeph>plc_python_shared</codeph>:</p>
        <codeblock>postgres=# CREATE FUNCTION dummyPython() RETURNS text AS $$
# container: plc_python_shared
return 'hello from Python'
$$ LANGUAGE plcontainer;</codeblock>
        <p>And test the function using:</p>
        <codeblock>postgres=# SELECT dummyPython();
    dummypython    
-------------------
 hello from Python
(1 row)
</codeblock>
        <p>Similarly, to test the R
          runtime:<codeblock>postgres=# CREATE FUNCTION dummyR() RETURNS text AS $$
# container: plc_r_shared
return ('hello from R')
$$ LANGUAGE plcontainer;
CREATE FUNCTION
postgres=# select dummyR();
    dummyr    
--------------
 hello from R
(1 row)</codeblock></p>
        <p>For further details and examples about using PL/Container functions, see <xref
            href="#topic_kwg_qfg_mjb">PL/Container Functions </xref>.</p>
      </section>
    </body>
  </topic>
 
  <topic id="upgrade_plcontainer">
    <title>Upgrade PL/Container</title>
    <body>
      <section>
        <title>Upgrading from PL/Container 1.1</title>
        <p>To upgrade to PL/Container 1.1 or later, uninstall version 1.0 and install the new
          version. You cannot use the <codeph>gppkg</codeph> option <codeph>-u</codeph>. The
            <codeph>gppkg</codeph> utility installs PL/Container 1.1 and later as a Greenplum
          Database extension, while PL/Container 1.0 is installed as a Greenplum Database language.
          The Docker images and the PL/Container configuration do not change when upgrading
          PL/Container, only the PL/Container extension installation changes.</p>
        <p>As part of the upgrade process, you must drop PL/Container from all databases that are
          configured with PL/Container.</p>
        <note type="important">Dropping PL/Container from a database drops all PL/Container UDFs
          from the database, including user-created PL/Container UDFs. If the UDFs are required,
          ensure you can re-create the UDFs before dropping PL/Container. This
            <codeph>SELECT</codeph> command lists the names of and body of PL/Container UDFs in a
            database.<codeblock>SELECT proname, prosrc FROM pg_proc WHERE prolang = (SELECT oid FROM pg_language WHERE lanname = 'plcontainer');</codeblock><p>For
            information about the catalog tables, <codeph>pg_proc</codeph> and
              <codeph>pg_language</codeph>, see <xref
              href="../system_catalogs/catalog_ref-tables.xml" format="dita"/>. </p></note>
        <p>These steps upgrade from PL/Container 1.0 to PL/Container 1.1 or later in a database. The
          steps save the PL/Container 1.0 configuration and restore the configuration for use with
          PL/Container 1.1 or later.<ol id="ol_axf_mmq_vcb">
            <li>Save the PL/Container configuration. This example saves the configuration to
                <codeph>plcontainer10-backup.xml</codeph> in the local
              directory.<codeblock>plcontainer runtime-backup -f plcontainer10-backup.xml</codeblock></li>
            <li>Remove any <codeph>setting</codeph> elements that contain the
                <codeph>use_container_network</codeph> attribute from the configuration file. For
              example, this <codeph>setting</codeph> element must be removed from the configuration
              file.<codeblock>&lt;setting use_container_network="yes"/></codeblock></li>
            <li>Run the <codeph>plcontainer_uninstall.sql</codeph> script as the
                <codeph>gpadmin</codeph> user for each database that is configured with
              PL/Container. For example, this command drops the <codeph>plcontainer</codeph>
              language in the <codeph>mytest</codeph> database.
                <codeblock>psql -d mytest -f $GPHOME/share/postgresql/plcontainer/plcontainer_uninstall.sql</codeblock><p>The
                script drops the <codeph>plcontainer</codeph> language with the
                  <codeph>CASCADE</codeph> clause that drops PL/Container-specific functions and
                views in the database. </p></li>
            <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the
                <codeph>-r</codeph> option to uninstall the PL/Container language extension. This
              example uninstalls the PL/Container language extension on a Linux
              system.<codeblock>$ gppkg -r plcontainer-1.0.0</codeblock></li>
            <li>Run the package installation command. This example installs the PL/Container 1.6.0
              language extension on a Linux
              system.<codeblock>gppkg -i plcontainer-1.6.0-rhel7-x86_64.gppkg</codeblock></li>
            <li>Source the file
              <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>source $GPHOME/greenplum_path.sh</codeblock></li>
            <li>Update the PL/Container configuration. This command restores the saved
              configuration.<codeblock>plcontainer runtime-restore -f plcontainer10-backup.xml</codeblock></li>
            <li>Restart Greenplum Database.<codeblock>gpstop -ra</codeblock></li>
            <li>Register the new PL/Container extension as an extension for each database that uses
              PL/Container UDFs. This <codeph>psql</codeph> command runs a <codeph>CREATE
                EXTENSION</codeph> command to register PL/Container in the database
                <codeph>mytest</codeph>.
                <codeblock>psql -d mytest -c 'CREATE EXTENSION plcontainer;'</codeblock><p>The
                command registers PL/Container as an extension and creates PL/Container-specific
                functions and views.</p></li>
          </ol></p>
        <p>After upgrading PL/Container for a database, re-install any user-created PL/Container
          UDFs that are required.</p>
      </section>
      <section>
        <title>Upgrading from PL/Container 1.0</title>
        <note>To upgrade to PL/Container 1.1 or later from PL/Container 1.0, you uninstall the old
          version and install the new version. </note>
        <p>To upgrade from PL/Container 1.1 or higher, you save the current configuration, upgrade
          PL/Container, and then restore the configuration after upgrade. There is no need to update
          the Docker images when you upgrade PL/Container.</p>
        <note>Before you perform this upgrade procedure, ensure that you have migrated your
          PL/Container 1.1 package from your previous Greenplum Database installation to your new
          Greenplum Database installation. Refer to the <xref
            href="../../utility_guide/admin_utilities/gppkg.html#topic1" format="dita" scope="peer"
            >gppkg</xref> command for package installation and migration information.</note>
        <p>Perform the following procedure to upgrade from PL/Container 1.1 to PL/Container version
          1.2 or later.<ol id="ol_upgrade">
            <li>Save the PL/Container configuration. For example, to save the configuration to a
              file named <codeph>plcontainer110-backup.xml</codeph> in the local
              directory:<codeblock>$ plcontainer runtime-backup -f plcontainer110-backup.xml</codeblock></li>
            <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the
                <codeph>-u</codeph> option to update the PL/Container language extension. For
              example, the following command updates the PL/Container language extension to version
              1.6.0 on a Linux
              system:<codeblock>$ gppkg -u plcontainer-1.6.0-rhel7-x86_64.gppkg</codeblock></li>
            <li>Source the Greenplum Database environment file
                <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>$ source $GPHOME/greenplum_path.sh</codeblock></li>
            <li>Restore the PL/Container configuration. For example, this command restores the
              PL/Container configuration that you saved in a previous step:
              <codeblock>$ plcontainer runtime-restore -f plcontainer110-backup.xml</codeblock></li>
            <li>Restart Greenplum Database.<codeblock>$ gpstop -ra</codeblock></li>
            <li>You do not need to re-register the PL/Container extension in the databases in which
              you previously created the extension. You must register the PL/Container extension in
              each new database that will run PL/Container UDFs. For example, the following command
              registers PL/Container in a database named <codeph>mytest</codeph>:
                <codeblock>$ psql -d mytest -c 'CREATE EXTENSION plcontainer;'</codeblock><p>The
                command also creates PL/Container-specific functions and views.</p></li>
          </ol></p>
        <note>PL/Container 1.2 and later utilizes the new resource group capabilities introduced in
          Greenplum Database 5.8.0. If you downgrade to a Greenplum Database system that uses
          PL/Container 1.1. or earlier, you must use <codeph>plcontainer runtime-edit</codeph> to
          remove any <codeph>resource_group_id</codeph> settings from your PL/Container runtime
          configuration.</note>
      </section>
    </body>
  </topic>
  <topic id="uninstall_plcontainer" xml:lang="en">
    <title id="pz213704">Uninstall PL/Container</title>
    <body>
      <p>To uninstall PL/Container, remove Docker containers and images, and then remove the
        PL/Container support from Greenplum Database.</p>
      <p>When you remove support for PL/Container, the <codeph>plcontainer</codeph> user-defined
        functions that you created in the database will no longer work. </p>
    </body>
    <topic id="topic_rnb_4s5_lw">
      <title>Uninstall Docker Containers and Images</title>
      <body>
        <p>On the Greenplum Database hosts, uninstall the Docker containers and images that are no
          longer required. </p>
        <p>The <codeph>plcontainer image-list</codeph> command lists the Docker images that are
          installed on the local Greenplum Database host. </p>
        <p>The <codeph>plcontainer image-delete</codeph> command deletes a specified Docker image
          from all Greenplum Database hosts. </p>
        <p>Some Docker containers might exist on a host if the containers were not managed by
          PL/Container. You might need to remove the containers with Docker commands. These
            <codeph>docker</codeph> commands manage Docker containers and images on a local host.<ul
            id="ul_emd_ts5_lw">
            <li>The command <codeph>docker ps -a</codeph> lists all containers on a host. The
              command <codeph>docker stop</codeph> stops a container.</li>
            <li>The command <codeph>docker images</codeph> lists the images on a host.</li>
            <li>The command <codeph>docker rmi</codeph> removes images.</li>
            <li>The command <codeph>docker rm</codeph> removes containers. </li>
          </ul></p>
      </body>
    </topic>
    <topic xml:lang="en" id="topic_qnb_3cj_kw">
      <title>Remove PL/Container Support for a Database</title>
      <body>
        <p>To remove support for PL/Container, drop the extension from the database. Use the
            <codeph>psql</codeph> utility with <codeph>DROP EXTENION</codeph> command (using
            <codeph>-c</codeph>) to remove PL/Container from <codeph>mytest</codeph> database.</p>
        <codeblock>psql -d mytest -c 'DROP EXTENSION plcontainer CASCADE;'</codeblock>
        <p>The <codeph>CASCADE</codeph> keyword drops PL/Container-specific functions and views.</p>
      </body>
    </topic>
    <topic xml:lang="en" id="topic_dty_fcj_kw" otherprops="pivotal">
      <title>Uninstall the PL/Container Language Extension</title>
      <body>
        <p>If no databases have <codeph>plcontainer</codeph> as a registered language, uninstall the
          Greenplum Database PL/Container language extension with the <codeph>gppkg</codeph>
          utility. </p>
        <ol id="ol_ety_fcj_kw">
          <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the <codeph>-r</codeph>
            option to uninstall the PL/Container language extension. This example uninstalls the
            PL/Container language extension on a Linux
              system:<codeblock>$ gppkg -r plcontainer-2.1.1</codeblock><p>You can run the
                <codeph>gppkg</codeph> utility with the options <codeph>-q --all</codeph> to list
              the installed extensions and their versions.</p></li>
          <li>Reload
            <codeph>greenplum_path.sh</codeph>.<codeblock>$ source $GPHOME/greenplum_path.sh</codeblock></li>
          <li>Restart the database.<codeblock>$ gpstop -ra</codeblock></li>
        </ol>
      </body>
    </topic>
  </topic>
  <topic id="use_plcontainer">
    <title>Using PL/Container</title>
    <body>
      <p>This topic covers further details on: </p>
      <p>
        <ul id="ul_ozt_t1j_blb">
          <li><xref href="#topic_resmgmt">PL/Container Resource Management</xref></li>
          <li><xref href="#topic_rh3_p3q_dw">PL/Container Functions</xref></li>
        </ul>
      </p>
    </body>
  </topic>
  <topic id="topic_resmgmt">
    <title>PL/Container Resource Management</title>
    <body>
      <section id="intro_resmgmt">
        <p>The Docker containers and the Greenplum Database servers share CPU and memory resources
          on the same hosts. In the default case, Greenplum Database is unaware of the resources
          consumed by running PL/Container instances. You can use Greenplum Database resource groups
          to control overall CPU and memory resource usage for running PL/Container instances.</p>
        <p>PL/Container manages resource usage at two levels - the container level and the runtime
          level. You can control container-level CPU and memory resources with the
            <codeph>memory_mb</codeph> and <codeph>cpu_share</codeph> settings that you configure
          for the PL/Container runtime. <codeph>memory_mb</codeph> governs the memory resources
          available to each container instance. The <codeph>cpu_share</codeph> setting identifies
          the relative weighting of a container's CPU usage compared to other containers. See <xref
            href="plcontainer-configuration.xml" format="dita"/> for further details.</p>
        <p>You cannot, by default, restrict the number of executing PL/Container container
          instances, nor can you restrict the total amount of memory or CPU resources that they
          consume.</p>
      </section>
      <section id="topic_resgroup">
        <title>Using Resource Groups to Manage PL/Container Resources</title>
        <p>With PL/Container 1.2.0 and later, you can use Greenplum Database resource groups to
          manage and limit the total CPU and memory resources of containers in PL/Container
          runtimes. For more information about enabling, configuring, and using Greenplum Database
          resource groups, refer to <xref href="../../admin_guide/workload_mgmt_resgroups.xml"
            format="dita" scope="peer">Using Resource Groups</xref> in the <cite>Greenplum Database
            Administrator Guide</cite>.</p>
        <note>If you do not explicitly configure resource groups for a PL/Container runtime, its
          container instances are limited only by system resources. The containers may consume
          resources at the expense of the Greenplum Database server.</note>
        <p>Resource groups for external components such as PL/Container use Linux control groups
          (cgroups) to manage component-level use of memory and CPU resources. When you manage
          PL/Container resources with resource groups, you configure both a memory limit and a CPU
          limit that Greenplum Database applies to all container instances that share the same
          PL/Container runtime configuration.</p>
        <p>When you create a resource group to manage the resources of a PL/Container runtime, you
          must specify <codeph>MEMORY_AUDITOR=cgroup</codeph> and <codeph>CONCURRENCY=0</codeph> in
          addition to the required CPU and memory limits. For example, the following command creates
          a resource group named <codeph>plpy_run1_rg</codeph> for a PL/Container runtime:
          <codeblock>CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
          CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></p>
        <p>PL/Container does not use the <codeph>MEMORY_SHARED_QUOTA</codeph> and
            <codeph>MEMORY_SPILL_RATIO</codeph> resource group memory limits. Refer to the
              <codeph><xref href="../../ref_guide/sql_commands/CREATE_RESOURCE_GROUP.xml"
              scope="peer">CREATE RESOURCE GROUP</xref></codeph> reference page for detailed
          information about this SQL command.</p>
        <p>You can create one or more resource groups to manage your running PL/Container instances.
          After you create a resource group for PL/Container, you assign the resource group to one
          or more PL/Container runtimes. You make this assignment using the <codeph>groupid</codeph>
          of the resource group. You can determine the <codeph>groupid</codeph> for a given resource
          group name from the <codeph>gp_resgroup_config</codeph>
          <codeph>gp_toolkit</codeph> view. For example, the following query displays the
            <codeph>groupid</codeph> of a resource group named
          <codeph>plpy_run1_rg</codeph>:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config
 WHERE groupname='plpy_run1_rg';
                            
 groupname   |  groupid
 --------------+----------
 plpy_run1_rg |   16391
 (1 row)</codeblock></p>
        <p>You assign a resource group to a PL/Container runtime configuration by specifying the
            <codeph>-s resource_group_id=<varname>rg_groupid</varname></codeph> option to the
            <codeph>plcontainer runtime-add</codeph> (new runtime) or <codeph>plcontainer
            runtime-replace</codeph> (existing runtime) commands. For example, to assign the
            <codeph>plpy_run1_rg</codeph> resource group to a new PL/Container runtime named
            <codeph>python_run1</codeph>:
          <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391</codeblock></p>
        <p>You can also assign a resource group to a PL/Container runtime using the
            <codeph>plcontainer runtime-edit</codeph> command. For information about the
            <codeph>plcontainer</codeph> command, see <xref href="plcontainer.xml" format="dita"/>
          reference page.</p>
        <p>After you assign a resource group to a PL/Container runtime, all container instances that
          share the same runtime configuration are subject to the memory limit and the CPU limit
          that you configured for the group. If you decrease the memory limit of a PL/Container
          resource group, queries executing in running containers in the group may fail with an out
          of memory error. If you drop a PL/Container resource group while there are running
          container instances, Greenplum Database kills the running containers.</p>
      </section>
      <section id="topic_resgroupcfg">
        <title>Configuring Resource Groups for PL/Container</title>
        <p>To use Greenplum Database resource groups to manage PL/Container resources, you must
          explicitly configure both resource groups and PL/Container.</p>
        <p>Perform the following procedure to configure PL/Container to use Greenplum Database
          resource groups for CPU and memory resource management:</p>
        <ol id="ol_qdq_vmh_blb">
          <li>If you have not already configured and enabled resource groups in your Greenplum
            Database deployment, configure cgroups and enable Greenplum Database resource groups as
            described in <xref href="../../admin_guide/workload_mgmt_resgroups.xml#topic71717999"
              format="dita" scope="peer">Using Resource Groups</xref> in the <cite>Greenplum
              Database Administrator Guide</cite>. <note>If you have previously configured and
              enabled resource groups in your deployment, ensure that the Greenplum Database
              resource group <codeph>gpdb.conf</codeph> cgroups configuration file includes a
                <codeph>memory { }</codeph> block as described in the previous link.</note></li>
          <li>Analyze the resource usage of your Greenplum Database deployment. Determine the
            percentage of resource group CPU and memory resources that you want to allocate to
            PL/Container Docker containers.</li>
          <li>Determine how you want to distribute the total PL/Container CPU and memory resources
            that you identified in the step above among the PL/Container runtimes. Identify:<ul
              id="ul_rdq_vmh_blb">
              <li>The number of PL/Container resource group(s) that you require.</li>
              <li>The percentage of memory and CPU resources to allocate to each resource
                group.</li>
              <li>The resource-group-to-PL/Container-runtime assignment(s).</li>
            </ul></li>
          <li>Create the PL/Container resource groups that you identified in the step above. For
            example, suppose that you choose to allocate 25% of both memory and CPU Greenplum
            Database resources to PL/Container. If you further split these resources among 2
            resource groups 60/40, the following SQL commands create the resource
            groups:<codeblock>CREATE RESOURCE GROUP plr_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                                   CPU_RATE_LIMIT=15, MEMORY_LIMIT=15);
 CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                                   CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></li>
          <li>Find and note the <codeph>groupid</codeph> associated with each resource group that
            you created. For
            example:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config
WHERE groupname IN ('plpy_run1_rg', 'plr_run1_rg');
                                    
groupname   |  groupid
--------------+----------
plpy_run1_rg |   16391
plr_run1_rg  |   16393
(1 row)</codeblock></li>
          <li>Assign each resource group that you created to the desired PL/Container runtime
            configuration. If you have not yet created the runtime configuration, use the
              <codeph>plcontainer runtime-add</codeph> command. If the runtime already exists, use
            the <codeph>plcontainer runtime-replace</codeph> or <codeph>plcontainer
              runtime-edit</codeph> command to add the resource group assignment to the runtime
            configuration. For example:
              <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391
plcontainer runtime-replace -r r_run1 -i pivotaldata/plcontainer_r_shared:devel -l r -s resource_group_id=16393</codeblock><p>For
              information about the <codeph>plcontainer</codeph> command, see <xref
                href="plcontainer.xml" format="dita"/> reference page.</p></li>
        </ol>
      </section>
      <section id="plc_notes">
        <title>Notes</title>
        <p id="pl_logging_notes"><b>PL/Container logging</b></p>
        <p>When PL/Container logging is enabled, you can set the log level with the Greenplum
          Database server configuration parameter <codeph><xref
              href="../../ref_guide/config_params/guc-list.xml#log_min_messages" scope="peer"
              >log_min_messages</xref></codeph>. The default log level is <codeph>warning</codeph>.
          The parameter controls the PL/Container log level and also controls the Greenplum Database
          log level.</p>
        <ul id="ul_knd_jhl_mcb">
          <li>PL/Container logging is enabled or disabled for each runtime ID with the
              <codeph>setting</codeph> attribute <codeph>use_container_logging</codeph>. The default
            is no logging. </li>
          <li>The PL/Container log information is the information from the UDF that is run in the
            Docker container. By default, the PL/Container log information is sent to a system
            service. On Red Hat 7 or CentOS 7 systems, the log information is sent to the
              <codeph>journald</codeph> service. </li>
          <li>The Greenplum Database log information is sent to log file on the Greenplum Database
            master.</li>
          <li>When testing or troubleshooting a PL/Container UDF, you can change the Greenplum
            Database log level with the <codeph>SET</codeph> command. You can set the parameter in
            the session before you run your PL/Container UDF. This example sets the log level to
              <codeph>debug1</codeph>. <codeblock>SET log_min_messages='debug1' ;</codeblock>
            <note>The parameter <codeph>log_min_messages</codeph> controls both the Greenplum
              Database and PL/Container logging, increasing the log level might affect Greenplum
              Database performance even if a PL/Container UDF is not running.</note></li>
        </ul>
      </section>
    </body>
  </topic>
  <topic id="topic_rh3_p3q_dw">
    <title>PL/Container Functions</title>
    <body>
      <p>When you enable PL/Container in a database of a Greenplum Database system, the language
          <codeph>plcontainer</codeph> is registered in that database. Specify
          <codeph>plcontainer</codeph> as a language in a UDF definition to create and run
        user-defined functions in the procedural languages supported by the PL/Container Docker
        images.</p>
      <section id="topic_c3v_clg_wkb">
        <title>Limitations</title>
        <p>Review the following limitations when creating and using PL/Container PL/Python and PL/R
          functions: </p>
        <ul id="ul_d3v_clg_wkb">
          <li>Greenplum Database domains are not supported.</li>
          <li>Multi-dimensional arrays are not supported.</li>
          <li>Python and R call stack information is not displayed when debugging a UDF.</li>
          <li>The <codeph>plpy.execute()</codeph> methods <codeph>nrows()</codeph> and
              <codeph>status()</codeph> are not supported.</li>
          <li>The PL/Python function <codeph>plpy.SPIError()</codeph> is not supported.</li>
          <li>Executing the SAVEPOINT command with <codeph>plpy.execute()</codeph> is not
            supported.</li>
          <li>The DO command is not supported.</li>
          <li>See .OUT parameters are not supported.The Python dict type cannot be returned from a
            PL/Python UDF. </li>
          <li>When returning the Python dict type from a UDF, you can convert the dict type to a
            Greenplum Database user-defined data type (UDT).</li>
        </ul>
      </section>
      <section id="using_functions">
        <title>Using PL/Container functions</title>
        <p>A UDF definition that uses PL/Container must have the these items.</p>
        <ul id="ul_z2m_1kj_kw">
          <li>The first line of the UDF must be <codeph># container:
            <varname>ID</varname></codeph></li>
          <li>The <codeph>LANGUAGE</codeph> attribute must be <codeph>plcontainer</codeph></li>
        </ul>
        <p>The <varname>ID</varname> is the name that PL/Container uses to identify a Docker image.
          When Greenplum Database executes a UDF on a host, the Docker image on the host is used to
          start a Docker container that runs the UDF. In the XML configuration file
            <codeph>plcontainer_configuration.xml</codeph>, there is a <codeph>runtime</codeph> XML
          element that contains a corresponding <codeph>id</codeph> XML element that specifies the
          Docker container startup information. See <xref
            href="plcontainer-configuration.xml#topic_sk1_gdq_dw" format="dita"/> for information
          about how PL/Container maps the <varname>ID</varname> to a Docker image. See <xref
            href="#function_examples" format="dita"/> for example UDF definitions.</p>
        <p>The PL/Container configuration file is read only on the first invocation of a
          PL/Container function in each Greenplum Database session that runs PL/Container functions.
          You can force the configuration file to be re-read by performing a <codeph>SELECT</codeph>
          command on the view <codeph>plcontainer_refresh_config</codeph> during the session. For
          example, this <codeph>SELECT</codeph> command forces the configuration file to be
          read.</p>
        <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
        <p>Running the command executes a PL/Container function that updates the configuration on
          the master and segment instances and returns the status of the
          refresh.<codeblock> gp_segment_id | plcontainer_refresh_local_config
 ---------------+----------------------------------
 1 | ok
 0 | ok
-1 | ok
(3 rows)</codeblock></p>
        <p>Also, you can show all the configurations in the session by performing a
            <codeph>SELECT</codeph> command on the view <codeph>plcontainer_show_config</codeph>.
          For example, this <codeph>SELECT</codeph> command returns the PL/Container configurations. </p>
        <codeblock>SELECT * FROM plcontainer_show_config;</codeblock>
        <p>Running the command executes a PL/Container function that displays configuration
          information from the master and segment instances. This is an example of the start and end
          of the view
          output.<codeblock>INFO:  plcontainer: Container 'plc_py_test' configuration
 INFO:  plcontainer:     image = 'pivotaldata/plcontainer_python_shared:devel'
 INFO:  plcontainer:     memory_mb = '1024'
 INFO:  plcontainer:     use container network = 'no'
 INFO:  plcontainer:     use container logging  = 'no'
 INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/./bin/plcontainer_clients' to container '/clientdir'
 INFO:  plcontainer:     access = readonly
                
 ...
                
 INFO:  plcontainer: Container 'plc_r_example' configuration  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     image = 'pivotaldata/plcontainer_r_without_clients:0.2'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     memory_mb = '1024'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     use container network = 'no'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     use container logging  = 'yes'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/bin/plcontainer_clients' to container '/clientdir'  (seg0 slice3 192.168.180.45:40000 pid=3304)
 INFO:  plcontainer:         access = readonly  (seg0 slice3 192.168.180.45:40000 pid=3304)
 gp_segment_id | plcontainer_show_local_config
 ---------------+-------------------------------
  0 | ok
 -1 | ok
  1 | ok</codeblock></p>
        <p>The PL/Container function <codeph>plcontainer_containers_summary()</codeph> displays
          information about the currently running Docker
          containers.<codeblock>SELECT * FROM plcontainer_containers_summary();</codeblock></p>
        <p>If a normal (non-superuser) Greenplum Database user runs the function, the function
          displays information only for containers created by the user. If a Greenplum Database
          superuser runs the function, information for all containers created by Greenplum Database
          users is displayed. This is sample output when 2 containers are running.</p>
        <codeblock> SEGMENT_ID |                           CONTAINER_ID                           |   UP_TIME    |  OWNER  | MEMORY_USAGE(KB)
 ------------+------------------------------------------------------------------+--------------+---------+------------------
 1          | 693a6cb691f1d2881ec0160a44dae2547a0d5b799875d4ec106c09c97da422ea | Up 8 seconds | gpadmin | 12940
 1          | bc9a0c04019c266f6d8269ffe35769d118bfb96ec634549b2b1bd2401ea20158 | Up 2 minutes | gpadmin | 13628
 (2 rows)</codeblock>
        <p>When Greenplum Database executes a PL/Container UDF, Query Executer (QE) processes start
          Docker containers and reuse them as needed. After a certain amount of idle time, a QE
          process quits and destroys its Docker containers. You can control the amount of idle time
          with the Greenplum Database server configuration parameter <codeph><xref
              href="../../ref_guide/config_params/guc-list.xml#gp_vmem_idle_resource_timeout"
              scope="peer">gp_vmem_idle_resource_timeout</xref></codeph>. Controlling the idle time
          might help with Docker container reuse and avoid the overhead of creating and starting a
          Docker container. </p>
        <note type="warning">Changing <codeph>gp_vmem_idle_resource_timeout</codeph> value, might
          affect performance due to resource issues. The parameter also controls the freeing of
          Greenplum Database resources other than Docker containers.</note>
      </section>
      <section xml:lang="en" id="function_examples">
        <title>Examples</title>
        <p>The values in the <codeph># container</codeph> lines of the examples,
            <codeph>plc_python_shared</codeph> and <codeph>plc_r_shared</codeph>, are the
            <codeph>id</codeph> XML elements defined in the <codeph>plcontainer_config.xml</codeph>
          file. The <codeph>id</codeph> element is mapped to the <codeph>image</codeph> element that
          specifies the Docker image to be started. If you configured PL/Container with a different
          ID, change the value of the <codeph># container</codeph> line. For information about
          configuring PL/Container and viewing the configuration settings, see <xref
            href="plcontainer-configuration.xml" format="dita"/>.</p>
        <p>This is an example of PL/Python function that runs using the
            <codeph>plc_python_shared</codeph> container that contains Python
          2:<codeblock>CREATE OR REPLACE FUNCTION pylog100() RETURNS double precision AS $$
 # container: plc_python_shared
 import math
 return math.log10(100)
 $$ LANGUAGE plcontainer;</codeblock></p>
        <p>This is an example of a similar function using the <codeph>plc_r_shared</codeph>
          container:<codeblock>CREATE OR REPLACE FUNCTION rlog100() RETURNS text AS $$
# container: plc_r_shared
return(log10(100))
$$ LANGUAGE plcontainer;</codeblock></p>
        <p>If the <codeph># container</codeph> line in a UDF specifies an ID that is not in the
          PL/Container configuration file, Greenplum Database returns an error when you try to
          execute the UDF.</p>
      </section>
      <section id="topic_ctk_xjg_wkb">
        <title>About PL/Container Running PL/Python </title>
        <p>In the Python language container, the module <codeph>plpy</codeph> is implemented. The
          module contains these methods:</p>
        <ul id="ul_dtk_xjg_wkb">
          <li><codeph>plpy.execute(stmt)</codeph> - Executes the query string <codeph>stmt</codeph>
            and returns query result in a list of dictionary objects. To be able to access the
            result fields ensure your query returns named fields.</li>
          <li><codeph>plpy.prepare(stmt[, argtypes])</codeph> - Prepares the execution plan for a
            query. It is called with a query string and a list of parameter types, if you have
            parameter references in the query.</li>
          <li><codeph>plpy.execute(plan[, argtypes])</codeph> - Executes a prepared plan.</li>
          <li><codeph>plpy.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum Database
            log.</li>
          <li><codeph>plpy.log(msg)</codeph> - Sends a LOG message to the Greenplum Database
            log.</li>
          <li><codeph>plpy.info(msg)</codeph> - Sends an INFO message to the Greenplum Database
            log.</li>
          <li><codeph>plpy.notice(msg)</codeph> - Sends a NOTICE message to the Greenplum Database
            log.</li>
          <li><codeph>plpy.warning(msg)</codeph> - Sends a WARNING message to the Greenplum Database
            log.</li>
          <li><codeph>plpy.error(msg)</codeph> - Sends an ERROR message to the Greenplum Database
            log. An ERROR message raised in Greenplum Database causes the query execution process to
            stop and the transaction to rollback.</li>
          <li><codeph>plpy.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum Database
            log. A FATAL message causes Greenplum Database session to be closed and transaction to
            be rolled back.</li>
          <li><codeph>plpy.subtransaction()</codeph> - Manages <codeph>plpy.execute</codeph> calls
            in an explicit subtransaction. See <xref
              href="https://www.postgresql.org/docs/9.4/plpython-subtransaction.html" format="html"
              scope="external">Explicit Subtransactions</xref> in the PostgreSQL documentation for
            additional information about <codeph>plpy.subtransaction()</codeph>.</li>
        </ul>
        <p>If an error of level <codeph>ERROR</codeph> or <codeph>FATAL</codeph> is raised in a
          nested Python function call, the message includes the list of enclosing functions.</p>
        <p>The Python language container supports these string quoting functions that are useful
          when constructing ad-hoc queries. <ul id="ul_etk_xjg_wkb">
            <li><codeph>plpy.quote_literal(string)</codeph> - Returns the string quoted to be used
              as a string literal in an SQL statement string. Embedded single-quotes and backslashes
              are properly doubled. <codeph>quote_literal()</codeph> returns null on null input
              (empty input). If the argument might be null, <codeph>quote_nullable()</codeph> might
              be more appropriate.</li>
            <li><codeph>plpy.quote_nullable(string)</codeph> - Returns the string quoted to be used
              as a string literal in an SQL statement string. If the argument is null, returns
                <codeph>NULL</codeph>. Embedded single-quotes and backslashes are properly
              doubled.</li>
            <li>
              <codeph>plpy.quote_ident(string)</codeph> - Returns the string quoted to be used as an
              identifier in an SQL statement string. Quotes are added only if necessary (for
              example, if the string contains non-identifier characters or would be case-folded).
              Embedded quotes are properly doubled. </li>
          </ul></p>
        <p>When returning text from a PL/Python function, PL/Container converts a Python unicode
          object to text in the database encoding. If the conversion cannot be performed, an error
          is returned.</p>
        <p>PL/Container does not support this Greenplum Database PL/Python feature:<ul
            id="ul_ftk_xjg_wkb">
            <li> Multi-dimensional arrays.</li>
          </ul></p>
        <p>Also, the Python module has two global dictionary objects that retain the data between
          function calls. They are named GD and SD. GD is used to share the data between all the
          function running within the same container, while SD is used for sharing the data between
          multiple calls of each separate function. Be aware that accessing the data is possible
          only within the same session, when the container process lives on a segment or master. Be
          aware that for idle sessions Greenplum Database terminates segment processes, which means
          the related containers would be shut down and the data from GD and SD lost.</p>
        <p>For information about PL/Python, see <xref href="pl_python.xml#topic1"/>. </p>
        <p>For information about the <codeph>plpy</codeph> methods, see <xref
            href="https://www.postgresql.org/docs/9.4/plpython-database.html" format="html"
            scope="external">https://www.postgresql.org/docs/9.4/plpython-database.htm</xref>. </p>
      </section>
      <section id="topic_lqz_t3q_dw">
        <title>About PL/Container Running PL/R</title>
        <p>In the R language container, the module <codeph>pg.spi</codeph> is implemented. The
          module contains these methods:</p>
        <ul id="ul_mqz_t3q_dw">
          <li><codeph>pg.spi.exec(stmt)</codeph> - Executes the query string <codeph>stmt</codeph>
            and returns query result in R <codeph>data.frame</codeph>. To be able to access the
            result fields make sure your query returns named fields.</li>
          <li><codeph>pg.spi.prepare(stmt[, argtypes])</codeph> - Prepares the execution plan for a
            query. It is called with a query string and a list of parameter types if you have
            parameter references in the query.</li>
          <li><codeph>pg.spi.execp(plan[, argtypes])</codeph> - Execute a prepared plan.</li>
          <li><codeph>pg.spi.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum Database
            log.</li>
          <li><codeph>pg.spi.log(msg)</codeph> - Sends a LOG message to the Greenplum Database
            log.</li>
          <li><codeph>pg.spi.info(msg)</codeph> - Sends an INFO message to the Greenplum Database
            log.</li>
          <li><codeph>pg.spi.notice(msg)</codeph> - Sends a NOTICE message to the Greenplum Database
            log.</li>
          <li><codeph>pg.spi.warning(msg)</codeph> - Sends a WARNING message to the Greenplum
            Database log.</li>
          <li><codeph>pg.spi.error(msg)</codeph> - Sends an ERROR message to the Greenplum Database
            log. An ERROR message raised in Greenplum Database causes the query execution process to
            stop and the transaction to rollback.</li>
          <li><codeph>pg.spi.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum Database
            log. A FATAL message causes Greenplum Database session to be closed and transaction to
            be rolled back.</li>
        </ul>
        <p>PL/Container does not support this PL/R feature:<ul id="ul_wjk_dgb_4cb">
            <li> Multi-dimensional arrays.</li>
          </ul></p>
        <p>For information about PL/R, see <xref href="pl_r.xml#topic1"/>.</p>
        <p>For information about the <codeph>pg.spi</codeph> methods, see <xref
            href="http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html" format="html"
            scope="external"
            >http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html</xref></p>
      </section>
    </body>
  </topic>
  <topic xml:lang="en" id="topic_kds_plk_rbb">
    <title>Docker References</title>
    <body>
      <p>Docker home page <xref href="https://www.docker.com/" format="html" scope="external"
          >https://www.docker.com/</xref></p>
      <p>Docker command line interface <xref
          href="https://docs.docker.com/engine/reference/commandline/cli/" format="html"
          scope="external">https://docs.docker.com/engine/reference/commandline/cli/</xref></p>
      <p>Dockerfile reference <xref href="https://docs.docker.com/engine/reference/builder/"
          format="html" scope="external"
        >https://docs.docker.com/engine/reference/builder/</xref></p>
      <p>For CentOS, see <xref href="https://docs.docker.com/engine/installation/linux/centos/"
          format="html" scope="external"> Docker site installation instructions for CentOS</xref>. </p>
      <p>For a list of Docker commands, see the <xref
          href="https://docs.docker.com/engine/reference/run/" format="html" scope="external">Docker
          engine Run Reference</xref>.</p>
      <p>Installing Docker on Linux systems <xref
          href="https://docs.docker.com/engine/installation/linux/centos/" format="html"
          scope="external">https://docs.docker.com/engine/installation/linux/centos/</xref></p>
      <p>Control and configure Docker with systemd <xref
          href="https://docs.docker.com/engine/admin/systemd/" format="html" scope="external"
          >https://docs.docker.com/engine/admin/systemd/</xref></p>
      <p>Changes to Python <xref href="https://docs.python.org/3/whatsnew/index.html" format="html"
          scope="external">Whats New in Python</xref></p>
      <p>Porting from Python 2 to 3 <xref href="https://docs.python.org/3/howto/pyporting.html"
          format="html" scope="external">Porting Python 2 Code to Python 3</xref></p>
    </body>
  </topic>
</topic>
